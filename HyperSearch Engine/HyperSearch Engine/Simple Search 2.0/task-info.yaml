type: edu
files:
- name: engine.py
  visible: true
  text: |
    # write your code here
  learner_created: false
- name: test/files/ex1/testfile1.txt
  visible: false
  text: |
    It seemed a delightful prospect. This man evidently understands my complaint.
  learner_created: false
- name: test/files/ex1/testfile2.txt
  visible: false
  text: |
    Every possible situation man could imagine has been spoilt.
  learner_created: false
- name: test/files/ex2/testfile3.txt
  visible: false
  text: |
    There's enough water to see, right?
    To tell the truth, it was water from the river, too.
  learner_created: false
- name: test/search.py
  visible: false
  text: "\n\"\"\"\nSearchEngine\nThis module does searching for positions of tokens\
    \ in database\n\"\"\"\nfrom test.tokenizator import Tokenizator\nfrom test.windows\
    \ import Context_Window\n\n\nclass SearchEngine(object):\n    \"\"\"\n    class\
    \ SearchEngine\n    \"\"\"\n    \n    def __init__(self):\n        \n        self.tokenizator\
    \ = Tokenizator()\n        \n          \n    def get_dict (self, database, tok_str):\n\
    \        \"\"\"\n        This function performs searching for positions of a given\
    \ token\n        @param tok_str: str containing token\n        @return: dictionary,\
    \ where a key is a filename\n        and a value is a list of positions\n    \
    \    \"\"\"\n        \n        if  not isinstance(tok_str,str):\n            raise\
    \ TypeError('Input has an unappropriate type!')\n        if  not isinstance(database,dict):\n\
    \            raise TypeError('Input has an unappropriate type!')\n\n        if\
    \ tok_str in database:\n            return database[tok_str]\n        else:\n\
    \            return {}\n\n\n    def get_dict_many_tokens(self, database, tok_str):\n\
    \        \"\"\"\n        This function performs searching for positions of given\
    \ tokens\n        @param tok_str: str containing tokens\n        @return: dictionary,\
    \ where a key is a filename\n        and a value is a list of positions of all\
    \ tokens     \n        \"\"\"\n\n        if not isinstance(tok_str, str):\n  \
    \          raise TypeError('Input has an unappropriate type!')\n        if not\
    \ tok_str:\n            return {}\n        big_dict_files = []\n        for token\
    \ in self.tokenizator.token_gen(tok_str):\n            big_dict_files.append(self.get_dict(database,token.s))\n\
    \            \n        files = set(big_dict_files[0])    \n        for file_dict\
    \ in big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \n        output_dict = {} \n        for filename in files:\n            for token\
    \ in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n\n    \n\
    \    def get_dict_many_tokens_limit_offset(self, database, tok_str, limit, offset):\n\
    \        \"\"\"\n        This function performs searching for positions of given\
    \ tokens\n        @param tok_str: str containing tokens\n        @param limit:\
    \ number of files to be returned\n        @param offset: from which file to start\n\
    \        @return: dictionary, where a key is a filename\n        @database: a\
    \ database where we search\n        and a value is a list of positions of all\
    \ tokens     \n        \"\"\"\n\n        if not isinstance(tok_str, str):\n  \
    \          raise TypeError('Input has an unappropriate type!')\n        \n   \
    \     if not isinstance(limit, int) or not isinstance (offset, int):\n       \
    \     raise TypeError('Input has an unappropriate type!')\n        \n        if\
    \ not tok_str:\n            return {}\n        \n        if offset < 0:\n    \
    \        offset = 0\n            \n        big_dict_files = []\n        for token\
    \ in self.tokenizator.token_gen(tok_str):\n            big_dict_files.append(self.get_dict(database,\
    \ token.s))  \n            \n        files = set(big_dict_files[0])    \n    \
    \    for file_dict in big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \               \n        resulted_files = sorted(files)[offset: limit+offset]\n\
    \        output_dict = {}\n        for filename in resulted_files:\n         \
    \   for token in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n    \n\
    \    def get_formated_result(self, database, tok_str, limit, offset):\n      \
    \  result = self.get_dict_many_tokens_limit_offset(database, tok_str, limit=3,\
    \ offset=0)\n        formated_result = '; '.join([f'{key}: {value}' for key, value\
    \ in result.items()])\n        return formated_result\n\n\n    def get_modified_search(self,\
    \ database, query, window_size, limit, offset):\n        simple_search_dict =\
    \ self.get_dict_many_tokens_limit_offset(database, query, limit, offset)\n   \
    \     windows = {}\n        for filename, positions_lists in simple_search_dict.items():\n\
    \            win_array = []\n            for position in positions_lists:\n  \
    \              window = Context_Window.get_window(filename, position, window_size)\n\
    \                win_array.append(window)\n            windows.setdefault(filename,[]).extend(win_array)\
    \                                                   \n        for filename, win_array\
    \ in windows.items():\n            for window in win_array:\n                window.extend_window()\n\
    \        for filename, win_array in windows.items():\n            i = 0\n    \
    \        while i < len(win_array)-1:\n                if win_array[i].is_crossed(win_array[i+1]):\n\
    \                    win_array[i].get_united_window(win_array[i+1])\n        \
    \            win_array.remove(win_array[i+1])\n                else:\n       \
    \             i+=1\n        output_dictionary = {}\n        for filename, win_array\
    \ in windows.items():\n            win_string_list = []\n            for window\
    \ in win_array:\n                win_string = window.string.strip() + '|' + str(window.positions)\
    \ + '|' + str(window.win_start) + '|' + str(window.win_end)\n                win_string_list.append(win_string)\n\
    \            output_dictionary.setdefault(filename,[]).extend(win_string_list)\n\
    \        return output_dictionary\n"
  learner_created: false
- name: test/indexer.py
  visible: false
  text: "\"\"\"\nIndexator\nThis module performs indexing of a text in a given file\n\
    \"\"\"\nfrom test.tokenizator import Tokenizator\nfrom functools import total_ordering\n\
    import os\n\nclass Position(object):\n    \"\"\"\n    Class Position\n    Cointains\
    \ positions of each token\n    \"\"\"\n\n    def __init__(self, start, end):\n\
    \        \"\"\"\n        @param start: position on the 1st element of a token\n\
    \        @param end: position on the last element of a token\n        \"\"\" \n\
    \        self.start = start\n        self.end = end\n        \n    def __eq__(self,\
    \ position):\n      \n        return self.start == position.start and self.end\
    \ == position.end\n\n    def __repr__(self):\n\n        return str(self.start)\
    \ + ',' + str(self.end)\n\n@total_ordering\nclass Position_Plus(Position):\n \
    \   \"\"\"\n    Class Position\n    Cointains positions of each token\n    \"\"\
    \"\n\n    def __init__(self, lnumber, start, end):\n        \"\"\"\n        @param\
    \ start: position on the 1st element of a token\n        @param end: position\
    \ on the last element of a token\n        @param lnumber: number of a line in\
    \ a given text\n        \"\"\"\n        \n        self.start = start\n       \
    \ self.end = end\n        self.lnumber = lnumber\n        \n    def __eq__(self,\
    \ position):\n\n        return self.lnumber == position.lnumber and self.start\
    \ == position.start and self.end == position.end \n\n    def __lt__(self, other_pos):\n\
    \        '''\n        This function compares two positions, i.e. their parameters\n\
    \        and returns the result of this comparison\n        @param other_pos:\
    \ position that is to be compared with self\n        @return: comparison_result,\
    \ i.e. True or False if one position less than another\n        '''\n        return\
    \ ((self.lnumber < other_pos.lnumber)\n                or ((self.lnumber == other_pos.lnumber)\n\
    \                and (self.start < other_pos.start)))\n\n    def __repr__(self):\n\
    \n        return str(self.lnumber) + ','+ str(self.start) + ',' + str(self.end)\n\
    \        \nclass Indexer_Dict(object):\n    def __init__(self):\n        \n  \
    \      self.tokenizator = Tokenizator()\n    \n    def get_index_dict(self, filedir):\n\
    \        \"\"\"\n        This function performs indexing of a text in a given\
    \ file\n        \"\"\"\n        if not isinstance(filedir, str):\n           \
    \ raise TypeError('Input has an unappropriate type!')\n        for filename in\
    \ os.listdir(filedir):\n            filename = os.path.join(filedir, filename)\n\
    \            my_file = open(filename)\n            output_dict = {}\n        \
    \    for lnumber,line in enumerate(my_file):\n                if not line:\n \
    \                   lnumber +=1\n                for token in self.tokenizator.token_gen(line):\n\
    \                    start = token.position\n                    end = start +\
    \ len(token.s)\n                    pos = Position_Plus(lnumber, start, end)\n\
    \                    output_dict.setdefault(token.s, {}).setdefault(filename,\
    \ []).append(pos)\n                lnumber +=1\n            my_file.close()\n\
    \            return output_dict\n    \n    def get_formated_result(self, filename):\n\
    \        result = self.get_index_dict(filename)\n        formated_result = ';\
    \ '.join([f'{key}: {value}' for key, value in result.items()])\n        return\
    \ formated_result\n"
  learner_created: false
- name: test/windows.py
  visible: false
  text: "\n\"\"\"\nContext_Windows\nThis module returns context windows for each query\
    \ word\n\"\"\"\nfrom test.tokenizator import Tokenizator\nimport re\n\nPATTERN_RIGHT\
    \ = re.compile(r'[\\.!?] [A-ZА-Яa-zа-я]') \nPATTERN_LEFT = re.compile(r'[A-ZА\
    -Яa-zа-я] [\\.!?]')\n\nclass Context_Window(object):\n    \"\"\"\n    class Context_Window\
    \ \n    \"\"\"\n    tokenizator = Tokenizator()\n    \n    def __init__(self,\
    \ string, positions, win_start, win_end):\n        \"\"\"\n        Constructor\
    \ of a context window\n        @param positions: positions of tokens\n       \
    \ @param string: string representation of a token\n        @param win_start: position\
    \ where window starts\n        @param win_end: position where window ends\n  \
    \      \"\"\"\n\n        self.string = string\n        self.positions = positions\n\
    \        self.win_start = win_start\n        self.win_end = win_end\n        \n\
    \        \n\n    def __eq__(self, window):\n\n         return self.string == window.string\
    \ and self.positions == window.positions and self.win_start == window.win_start\
    \ and self.win_end == window.win_end\n        \n    def __repr__(self):\n    \
    \    \n        return str(self.string) + ' ' + str(self.positions) + ' ' + str(self.win_start)\
    \ + ' ' + str(self.win_end)\n        \n    @classmethod\n    def get_window(cls,\
    \ filename, position, win_size):\n        \"\"\"\n        This function returns\
    \ a context window of a given token's position\n        @param filename: a name\
    \ of a file where token is to be found\n        @param position: a position of\
    \ a token\n        @param win_size: desirable size of the context window\n   \
    \     @return: a context window\n        \"\"\"\n        if not isinstance(filename,\
    \ str) or not isinstance(win_size, int):\n            raise TypeError('Input has\
    \ an unappropriate type! %s, %s' % (filename, win_size))\n        positions =\
    \ []\n        position = [position.lnumber,position.start,position.end]\n    \
    \    positions.append(position)\n        win_end = 0\n        win_start = 0\n\
    \        string = None\n        str_num = position[0]\n        my_file = open(filename)\n\
    \        for lnumber,my_string in enumerate(my_file):\n            if lnumber\
    \ == str_num:\n                string = my_string\n                break\n   \
    \         \n        if string == None:\n            my_file.close() \n       \
    \     raise TypeError('This string was not found!')\n            \n        for\
    \ tok_num,token in enumerate (cls.tokenizator.token_gen(string[position[1]:])):\n\
    \            if tok_num == 0:\n                win_end = position[2]\n       \
    \     if tok_num == win_size:\n                win_end = token.position + len(token.s)\
    \ + position[1]\n                break\n            \n        for tok_num,token\
    \ in enumerate (cls.tokenizator.token_gen(string[:position[2]][::-1])):\n    \
    \        if tok_num == win_size:\n                win_start = position[2] - token.position\
    \ - len(token.s)   \n                break\n            \n        my_file.close()\
    \    \n        return cls(string, positions, win_start, win_end)\n    \n    @classmethod\n\
    \    def get_formated_result(cls, filename, position, win_size):\n        if win_size\
    \ != 0:\n            result = cls.get_window(filename, position, win_size)\n \
    \           string = result.string\n            positions = str(result.positions)\n\
    \            start = str(result.win_start)\n            end = str(result.win_end)\n\
    \            formated_result = string.strip()+'|'+positions+'|'+start+'|'+end\n\
    \            return formated_result\n        else:\n            return ''\n\n\
    \    def get_formated_window(self, w2):\n        if not isinstance(w2, Context_Window):\n\
    \            raise TypeError('Input has an unappropriate type!')\n        if self.is_crossed(w2):\n\
    \            self.get_united_window(w2)\n            self.extend_window()\n  \
    \          highlighted = self.highlight_window()\n            formated_window\
    \ = highlighted.strip()+'|'+str(self.positions)+'|'+str(self.win_start)+'|'+str(self.win_end)\n\
    \            return formated_window\n        else:\n            self.extend_window()\n\
    \            w2.extend_window()\n            highlighted_win1 = self.highlight_window()\n\
    \            highlighted_win2 = w2.highlight_window()\n            formated_win1\
    \ = highlighted_win1.strip()+'|'+str(self.positions)+'|'+str(self.win_start)+'|'+str(self.win_end)\n\
    \            formated_win2 = highlighted_win2.strip()+'|'+str(w2.positions)+'|'+str(w2.win_start)+'|'+str(w2.win_end)\n\
    \            result = formated_win1 + '\\n ' + formated_win2\n            return\
    \ result\n        \n    def is_crossed(self, window_B):\n        '''\n       \
    \ This function checks if windows are crossed\n        @param window_B: the second\
    \ window\n        @return: True or False\n        '''\n        if not isinstance(window_B,\
    \ Context_Window):\n            raise TypeError('Input has an unappropriate type!')\n\
    \        if self.win_start <= window_B.win_end and self.win_end >= window_B.win_start\
    \ and self.positions[0][0] == window_B.positions[0][0]:\n            return True\n\
    \        if self.win_start == window_B.win_start and self.win_end == window_B.win_end\
    \ and self.positions[0][0] == window_B.positions[0][0]:\n            return True\n\
    \        else:\n            return False\n        \n    def get_united_window(self,\
    \ window_B):\n        \n        '''\n        This function unites two windows\n\
    \        @param window_B: the second window\n        It changes self so that is\
    \ has new positions and returns nothing!!\n        '''\n        \n        if not\
    \ isinstance(window_B, Context_Window):\n            raise TypeError('Input has\
    \ an unappropriate type!')\n        \n        self.positions.extend(window_B.positions)\n\
    \        self.win_start = min(window_B.win_start,self.win_start)\n        self.win_end\
    \ = max(window_B.win_end,self.win_end)\n       \n\n    def extend_window(self):\n\
    \        '''\n        This function extends a given window to sentence\n     \
    \   @return: an extended window\n        ''' \n        to_right = self.string[self.win_end:]\n\
    \        to_left = self.string[:self.win_start+1][::-1]\n        left = PATTERN_LEFT.search(to_left)\n\
    \        right = PATTERN_RIGHT.search(to_right)\n        if left is None:\n  \
    \          self.win_start = 0\n        else:\n            self.win_start -= left.start()\n\
    \        if right is None:\n            self.win_end = len(self.string)\n    \
    \    else:\n            self.win_end += right.start() + 1\n                  \
    \     \n\n    def highlight_window(self):\n        '''\n        This function\
    \ takes a substring of window string,\n        which corresponds to the window\
    \ size and highlights it \n        '''\n        win_string = self.string[self.win_start:self.win_end]\n\
    \        fin = '</b>'\n        st = '<b>'\n        for position in reversed(self.positions):\n\
    \            end = position[2] - self.win_start\n            begin = position[1]\
    \ - self.win_start\n            win_string_one = win_string[:end] + fin + win_string[end:]\n\
    \            win_string = win_string_one[:begin] + st + win_string_one[begin:]\n\
    \        return win_string\n\n\n"
  learner_created: false
- name: test/tokenizator.py
  visible: false
  text: "import unicodedata\n\"\"\"\nTokenizator\nThis module performs the morphological\
    \ analyses of a text and extracts tokens\n\"\"\"\nclass Token(object):\n    \"\
    \"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def __init__(self,position,s):\n\
    \        \"\"\"\n        Consrtuctor for token.\n        @param self: self is\
    \ an odject(here:token) with attributes\n        @param position: position is\
    \ an index of the first element of token\n        @param s: s is a string view\
    \ of a token\n        @return: token\n        \"\"\"\n        self.position=position\
    \  \n        self.s=s\n        \n    def __repr__(self):\n        \"\"\"\n   \
    \     The way the programm returns the final result.\n        \"\"\"\n       \
    \ return self.s+'_'+str(self.position)\n    \n\n    \nclass Token_Type(Token):\n\
    \    \"\"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def\
    \ __init__(self,s,tp,position):\n        \"\"\"\n        Consrtuctor for token.\n\
    \        @param self: self is an odject(here:token) with attributes\n        @param\
    \ s: s is a string view of a token\n        @param tp: tp is a type of a token\n\
    \        @return: token with it's type\n        \"\"\"\n        self.s=s\n   \
    \     self.tp=tp\n        self.position=position\n        \n    def __repr__(self):\n\
    \        \"\"\"\n        The way the programm returns the final result.\n    \
    \    \"\"\"\n        return self.s+ '_' + self.tp+'_'+ str(self.position)\n  \
    \      \n    \n    \nclass Tokenizator(object):\n    \"\"\"\n    Class that returns\
    \ tokens\n    \"\"\"\n    def tokenize(self,stream):\n        \"\"\"\n       \
    \ This is a function.\n        @param stream:stream is a text given\n        @return:\
    \ a list of tokens from the given text\n        \"\"\"\n        if  not isinstance(stream,str):\n\
    \            raise ValueError('Input has an unappropriate type, it should be str')\n\
    \        tokensback = []\n        for i,c in enumerate(stream): \n           \
    \ if c.isalpha() and (not stream[i-1].isalpha() or i==0):   \n               \
    \ position=i\n            if not c.isalpha()and i>0 and stream[i-1].isalpha():\
    \   \n                s=stream[position:i]\n                t=Token(position,s)\
    \                        \n                tokensback.append(t)      \n      \
    \  if c.isalpha():                   \n            s=stream[position:i+1]\n  \
    \          t=Token(position,s)\n            tokensback.append(t)\n        return\
    \ tokensback\n    \n    def tokens_generator(self, stream):\n        \"\"\"\n\
    \        This is a generator.\n        @param stream:stream is a text given\n\
    \        @return: tokens from the given text\n        \"\"\"\n        if  not\
    \ isinstance(stream, str):\n            raise ValueError('Input has an unappropriate\
    \ type, it should be str')\n        for i,c in enumerate(stream):  \n        \
    \    if c.isalpha() and (not stream[i-1].isalpha() or i == 0):   \n          \
    \      position = i\n            if not c.isalpha()and i>0 and stream[i-1].isalpha():\
    \   \n                s = stream[position:i]\n                t = Token(position,\
    \ s)                        \n                yield(t)    \n        if c.isalpha():\
    \                   \n            s = stream[position:i+1]\n            t = Token(position,\
    \ s)\n            yield(t)\n           \n    @staticmethod\n    def tokens_type_definition(x):\n\
    \       \"\"\"\n       This is a static method, which defines a type of a token\n\
    \       @return: type of a token\n       \"\"\"\n       tp = 'type' \n       if\
    \ x.isalpha():\n           tp = 'alpha'\n       if x.isdigit():\n           tp\
    \ = 'digit'\n       if x.isspace():\n           tp = 'space'\n       if unicodedata.category(x)[0]\
    \ == 'P':\n           tp = 'punct'\n       return tp      \n      \n    def tokens_generator_plus_type\
    \ (self, stream):\n        \"\"\"\n        This is a generator.\n        @param\
    \ stream:stream is a text given\n        @return: tokens from the given text plus\
    \ their type\n        \"\"\"\n        if  not isinstance(stream, str):\n     \
    \       raise ValueError('Input has an unappropriate type, it should be str')\n\
    \        position=0\n        for i,c in enumerate(stream):  \n            if self.tokens_type_definition(c)\
    \ != self.tokens_type_definition(stream[i-1]) and i>0:\n                tp = self.tokens_type_definition(stream[i-1])\n\
    \                s = stream[position:i]\n                position = i\n      \
    \          t = Token_Type(s,tp,position)                        \n           \
    \     yield(t)      \n        if self.tokens_type_definition(c):\n           \
    \ tp = self.tokens_type_definition(c)\n            s = stream[position:i+1]\n\
    \            t = Token_Type(s,tp,position)\n            yield(t)\n           \
    \ \n    def tokens_generator_plus_type_optimized (self, stream):\n        \"\"\
    \"\n        This is a generator.\n        @param stream:stream is a text given\n\
    \        @return: tokens from the given text plus their type\n        \"\"\"\n\
    \        if  not isinstance(stream, str):\n            raise ValueError('Input\
    \ has an unappropriate type, it should be str')\n        position=0\n        tp_of_c=self.tokens_type_definition(stream[0])\n\
    \        for i,c in enumerate(stream): \n            if i>0 and self.tokens_type_definition(c)\
    \ != tp_of_c:\n                tp = tp_of_c\n                tp_of_c = self.tokens_type_definition(c)\n\
    \                s = stream[position:i]\n                t = Token_Type(s,tp,position)\
    \  \n                position = i\n                yield(t)    \n        tp =\
    \ self.tokens_type_definition(c)\n        s = stream[position:i+1]\n        t\
    \ = Token_Type(s,tp,position)\n        yield(t)\n        \n    def token_gen(self,\
    \ stream):\n        if not stream:\n            print('None')\n        if len(stream)\
    \ == 0:\n            yield None\n        for token in self.tokens_generator_plus_type_optimized(stream):\n\
    \            if token.tp == 'alpha' or token.tp == 'digit':\n                yield(token)\n\
    \n    def token_gen_format(self, stream):\n        token_string = ''\n       \
    \ for token in self.tokens_generator_plus_type_optimized(stream):\n          \
    \  if token.tp == 'alpha' or token.tp == 'digit':\n                token_format\
    \ = token.s+'_'+token.tp+'_'+ str(token.position)\n                token_string+=\
    \ token_format+'\\n'\n        return token_string\n       \nif __name__ == '__main__':\n\
    \    x=Tokenizator()\n    \n"
  learner_created: false
- name: tests.py
  visible: false
  text: |
    from hstest.stage_test import *
    from hstest.test_case import TestCase
    from test.indexer import Indexer_Dict
    from test.search import SearchEngine
    import re
    import os

    CheckResult.correct = lambda: CheckResult(True, '')
    CheckResult.wrong = lambda feedback: CheckResult(False, feedback)

    x = SearchEngine()
    ind = Indexer_Dict()

    dir1 = os.path.join("test", "files", "ex1")
    dir2 = os.path.join("test", "files", "ex2")

    query1 = "man"
    query2 = "water"

    params1 = [1, 1, 0]
    params2 = [1, 2, 0]

    input1 = f"{dir1}\n'{query1}',{','.join([str(elem) for elem in params1])}"
    input2 = f"{dir2}\n'{query2}',{','.join([str(elem) for elem in params2])}"


    class SearchTest(StageTest):
        def generate(self):
            return [TestCase(stdin=input1,
                             attach=x.get_modified_search(ind.get_index_dict(dir1),
                                                          query=query1, window_size=params1[0],
                                                          limit=params1[1], offset=params1[2])),
                    TestCase(stdin=input2,
                             attach=x.get_modified_search(ind.get_index_dict(dir2),
                                                          query=query2, window_size=params2[0],
                                                          limit=params2[1], offset=params2[2]))]

        def check(self, reply: str, clue: Any) -> CheckResult:
            reply = reply.strip()
            reply = re.sub(r"['\"\[\]]", "", reply)

            for key, value in clue.items():
                key = os.path.split(key)[-1]
                if key not in reply:
                    return CheckResult.wrong("Seems like one of the positions is missing.\n"
                                             f"Position {key} was not found in your output.")
                expected_line = f"{key}: {str(value)}"
                expected_line_modified = re.sub(r"['\"\[\]]", "", expected_line)
                if expected_line_modified not in reply:
                    return CheckResult.wrong(f"Token positions \"{expected_line}\" were expected in the output,\n"
                                             f"but was not found.")
            return CheckResult.correct()


    if __name__ == '__main__':
        SearchTest().run_tests()
  learner_created: false
- name: test/__init__.py
  visible: false
  learner_created: false
- name: db.bak
  visible: true
  learner_created: true
- name: db.dir
  visible: true
  learner_created: true
feedback_link: https://hyperskill.org/projects/168/stages/874/implement#comment
status: Failed
feedback:
  message: |-
    Exception in test #1

    Traceback (most recent call last):
      File "engine.py", line 190, in <module>
        main()
      File "engine.py", line 186, in main
        tokens.upg_context(input(), input())
      File "engine.py", line 161, in upg_context
        text_0, position_0, start_0, end_0 = read_query(query_0)
      File "engine.py", line 135, in read_query
        tx, pos, w_st, w_end = qr.split('|')
    ValueError: not enough values to unpack (expected 4, got 1)

    Please find below the output of your program during this failed test.
    Note that the '>' character indicates the beginning of the input line.

    ---

    > test\files\ex1
    > 'man',1,1,0
  time: Wed, 07 Apr 2021 05:51:11 UTC
record: 6
