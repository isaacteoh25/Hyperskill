type: edu
files:
- name: engine.py
  visible: true
  text: |
    # write your code here
  learner_created: false
- name: test/search.py
  visible: false
  text: "\n\"\"\"\nSearchEngine\nThis module does searching for positions of tokens\
    \ in database\n\"\"\"\nfrom test.tokenizator import Tokenizator\n\n\nclass SearchEngine(object):\n\
    \    \"\"\"\n    class SearchEngine\n    \"\"\"\n    \n    def __init__(self):\n\
    \n        self.tokenizator = Tokenizator()\n        \n          \n    def get_dict(self,\
    \ database, tok_str):\n        \"\"\"\n        This function performs searching\
    \ for positions of a given token\n        @param tok_str: str containing token\n\
    \        @return: dictionary, where a key is a filename\n        and a value is\
    \ a list of positions\n        \"\"\"\n        \n        if  not isinstance(tok_str,str):\n\
    \            raise TypeError('Input has an unappropriate type!')\n        if \
    \ not isinstance(database,dict):\n            raise TypeError('Input has an unappropriate\
    \ type!')\n\n        if tok_str in database:\n            return database[tok_str]\n\
    \        else:\n            return {}\n\n\n    def get_dict_many_tokens(self,\
    \ database, tok_str):\n        \"\"\"\n        This function performs searching\
    \ for positions of given tokens\n        @param tok_str: str containing tokens\n\
    \        @return: dictionary, where a key is a filename\n        and a value is\
    \ a list of positions of all tokens     \n        \"\"\"\n\n        if not isinstance(tok_str,\
    \ str):\n            raise TypeError('Input has an unappropriate type!')\n   \
    \     if not tok_str:\n            return {}\n        big_dict_files = []\n  \
    \      for token in self.tokenizator.token_gen(tok_str):\n            big_dict_files.append(self.get_dict(database,token.s))\n\
    \            \n        files = set(big_dict_files[0])    \n        for file_dict\
    \ in big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \n        output_dict = {} \n        for filename in files:\n            for token\
    \ in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n\n    \n\
    \    def get_dict_many_tokens_limit_offset(self, database, tok_str, limit, offset):\n\
    \        \"\"\"\n        This function performs searching for positions of given\
    \ tokens\n        @param tok_str: str containing tokens\n        @param limit:\
    \ number of files to be returned\n        @param offset: from which file to start\n\
    \        @return: dictionary, where a key is a filename\n        @database: a\
    \ database where we search\n        and a value is a list of positions of all\
    \ tokens     \n        \"\"\"\n\n        if not isinstance(tok_str, str):\n  \
    \          raise TypeError('Input has an unappropriate type!')\n        \n   \
    \     if not isinstance(limit, int) or not isinstance (offset, int):\n       \
    \     raise TypeError('Input has an unappropriate type!')\n        \n        if\
    \ not tok_str:\n            return {}\n\n        if offset < 0:\n            offset\
    \ = 0\n            \n        big_dict_files = []\n        for token in self.tokenizator.token_gen(tok_str):\n\
    \            big_dict_files.append(self.get_dict(database, token.s))  \n     \
    \       \n        files = set(big_dict_files[0])    \n        for file_dict in\
    \ big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \               \n        resulted_files = sorted(files)[offset: limit+offset]\n\
    \        output_dict = {}\n        for filename in resulted_files:\n         \
    \   for token in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n    \n\
    \    def get_formated_result(self, database, tok_str, limit, offset):\n      \
    \  result = self.get_dict_many_tokens_limit_offset(database, tok_str, limit=limit,\
    \ offset=offset)\n        formated_result = '; '.join([f'{key}: {value}' for key,\
    \ value in result.items()])\n        return formated_result\n     \n"
  learner_created: false
- name: test/indexer.py
  visible: false
  text: "\"\"\"\nIndexator\nThis module performs indexing of a text in a given file\n\
    \"\"\"\nfrom test.tokenizator import Tokenizator\nfrom functools import total_ordering\n\
    \n@total_ordering\nclass Position_Plus(object):\n    \"\"\"\n    Class Position\n\
    \    Cointains positions of each token\n    \"\"\"\n\n    def __init__(self, lnumber,\
    \ start, end):\n        \"\"\"\n        @param start: position on the 1st element\
    \ of a token\n        @param end: position on the last element of a token\n  \
    \      @param lnumber: number of a line in a given text\n        \"\"\"\n    \
    \    \n        self.start = start\n        self.end = end\n        self.lnumber\
    \ = lnumber\n        \n    def __eq__(self, position):\n\n        return self.lnumber\
    \ == position.lnumber and self.start == position.start and self.end == position.end\
    \ \n\n    def __lt__(self, other_pos):\n        '''\n        This function compares\
    \ two positions, i.e. their parameters\n        and returns the result of this\
    \ comparison\n        @param other_pos: position that is to be compared with self\n\
    \        @return: comparison_result, i.e. True or False if one position less than\
    \ another\n        '''\n        return ((self.lnumber < other_pos.lnumber)\n \
    \               or ((self.lnumber == other_pos.lnumber)\n                and (self.start\
    \ < other_pos.start)))\n\n    def __repr__(self):\n\n        return str(self.lnumber)\
    \ + ','+ str(self.start) + ',' + str(self.end)\n        \nclass Indexer_Dict(object):\n\
    \    def __init__(self):\n        \n        self.tokenizator = Tokenizator()\n\
    \        \n    \n    def get_index_dict(self, filename):\n        \"\"\"\n   \
    \     This function performs indexing of a text in a given file\n        \"\"\"\
    \n        if  not isinstance(filename, str):\n            raise TypeError('Input\
    \ has an unappropriate type!')\n        my_file = open(filename)\n        output_dict\
    \ = {}\n        for lnumber,line in enumerate(my_file):\n            if not line:\n\
    \                lnumber+=1\n            for token in self.tokenizator.token_gen(line):\n\
    \                start = token.position\n                end = start + len(token.s)\n\
    \                pos = Position_Plus(lnumber, start, end)\n                output_dict.setdefault(token.s,\
    \ {}).setdefault(filename, []).append(pos)\n            lnumber+=1\n        my_file.close()\n\
    \        return output_dict\n    \n    def get_formated_result(self, filename):\n\
    \        result = self.get_index_dict(filename)\n        formated_result = ';\
    \ '.join([f'{key}: {value}' for key, value in result.items()])\n        return\
    \ formated_result\n"
  learner_created: false
- name: test/windows.py
  visible: false
  text: "\"\"\"\nContext_Windows\nThis module returns context windows for each query\
    \ word\n\"\"\"\nfrom test.tokenizator import Tokenizator\nimport re\n\nPATTERN_RIGHT\
    \ = re.compile(r'[\\.!?] [A-ZА-Яa-zа-я]') \nPATTERN_LEFT = re.compile(r'[A-ZА\
    -Яa-zа-я] [\\.!?]')\n\nclass Context_Window(object):\n    \"\"\"\n    class Context_Window\
    \ \n    \"\"\"\n    tokenizator = Tokenizator()\n    \n    def __init__(self,\
    \ string, positions, win_start, win_end):\n        \"\"\"\n        Constructor\
    \ of a context window\n        @param positions: positions of tokens\n       \
    \ @param string: string representation of a token\n        @param win_start: position\
    \ where window starts\n        @param win_end: position where window ends\n  \
    \      \"\"\"\n\n        self.string = string\n        self.positions = positions\n\
    \        self.win_start = win_start\n        self.win_end = win_end\n\n    def\
    \ __eq__(self, window):\n\n         return self.string == window.string and self.positions\
    \ == window.positions and self.win_start == window.win_start and self.win_end\
    \ == window.win_end\n        \n    def __repr__(self):\n        \n        return\
    \ str(self.string) + ' ' + str(self.positions) + ' ' + str(self.win_start) + '\
    \ ' + str(self.win_end)\n        \n    @classmethod\n    def get_window(cls, filename,\
    \ position, win_size):\n        \"\"\"\n        This function returns a context\
    \ window of a given token's position\n        @param filename: a name of a file\
    \ where token is to be found\n        @param position: a position of a token\n\
    \        @param win_size: desirable size of the context window\n        @return:\
    \ a context window\n        \"\"\"\n        if not isinstance(filename, str) or\
    \ not isinstance(win_size, int):\n            raise TypeError('Input has an unappropriate\
    \ type! %s, %s' % (filename, win_size))\n        positions = []\n        positions.append(position)\n\
    \        win_end = 0\n        win_start = 0\n        string = None\n        str_num\
    \ = position[0]\n        my_file = open(filename)\n        for lnumber,my_string\
    \ in enumerate(my_file):\n            if lnumber == str_num:\n               \
    \ string = my_string\n                break\n            \n        if string ==\
    \ None:\n            my_file.close() \n            raise TypeError('This string\
    \ was not found!')\n            \n        for tok_num,token in enumerate (cls.tokenizator.token_gen(string[position[1]:])):\n\
    \            if tok_num == 0:\n                win_end = position[2]\n       \
    \     if tok_num == win_size:\n                win_end = token.position + len(token.s)\
    \ + position[1]\n                break\n            \n        for tok_num,token\
    \ in enumerate (cls.tokenizator.token_gen(string[:position[2]][::-1])):\n    \
    \        if tok_num == win_size:\n                win_start = position[2] - token.position\
    \ - len(token.s)   \n                break\n            \n        my_file.close()\
    \    \n        return cls(string, positions, win_start, win_end)\n    \n    @classmethod\n\
    \    def get_formated_result(cls, filename, position, win_size):\n        if win_size\
    \ != 0:\n            result = cls.get_window(filename, position, win_size)\n \
    \           string = result.string\n            positions = str(result.positions)\n\
    \            start = str(result.win_start)\n            end = str(result.win_end)\n\
    \            formated_result = string.strip()+'|'+positions+'|'+start+'|'+end\n\
    \            return formated_result\n        else:\n            return ''\n  \
    \  \n    def get_formated_window(self, w2):\n        if not isinstance(w2, Context_Window):\n\
    \            raise TypeError('Input has an unappropriate type!')\n        if self.is_crossed(w2):\n\
    \            self.get_united_window(w2)\n            self.extend_window()\n  \
    \          highlighted = self.highlight_window()\n            formated_window\
    \ = highlighted.strip()+'|'+str(self.positions)+'|'+str(self.win_start)+'|'+str(self.win_end)\n\
    \            return formated_window\n        else:\n            self.extend_window()\n\
    \            w2.extend_window()\n            highlighted_win1 = self.highlight_window()\n\
    \            highlighted_win2 = w2.highlight_window()\n            formated_win1\
    \ = highlighted_win1.strip()+'|'+str(self.positions)+'|'+str(self.win_start)+'|'+str(self.win_end)\n\
    \            formated_win2 = highlighted_win2.strip()+'|'+str(w2.positions)+'|'+str(w2.win_start)+'|'+str(w2.win_end)\n\
    \            result = formated_win1 + ' \\n' + formated_win2\n            return\
    \ result\n        \n    def is_crossed(self, window_B):\n        '''\n       \
    \ This function checks if windows are crossed\n        @param window_B: the second\
    \ window\n        @return: True or False\n        '''\n        if not isinstance(window_B,\
    \ Context_Window):\n            raise TypeError('Input has an unappropriate type!')\n\
    \        if self.win_start <= window_B.win_end and self.win_end >= window_B.win_start\
    \ and self.positions[0][0] == window_B.positions[0][0]:\n            return True\n\
    \        if self.win_start == window_B.win_start and self.win_end == window_B.win_end\
    \ and self.positions[0][0] == window_B.positions[0][0]:\n            return True\n\
    \        else:\n            return False\n        \n    def get_united_window(self,\
    \ window_B):\n        \n        '''\n        This function unites two windows\n\
    \        @param window_B: the second window\n        It changes self so that is\
    \ has new positions and returns nothing!!\n        '''\n        \n        if not\
    \ isinstance(window_B, Context_Window):\n            raise TypeError('Input has\
    \ an unappropriate type!')\n        \n        self.positions.extend(window_B.positions)\n\
    \        self.win_start = min(window_B.win_start,self.win_start)\n        self.win_end\
    \ = max(window_B.win_end,self.win_end)\n\n    def extend_window(self):\n     \
    \   '''\n        This function extends a given window to sentence\n        @return:\
    \ an extended window\n        ''' \n        to_right = self.string[self.win_end:]\n\
    \        to_left = self.string[:self.win_start+1][::-1]\n        left = PATTERN_LEFT.search(to_left)\n\
    \        right = PATTERN_RIGHT.search(to_right)\n        if left is None:\n  \
    \          self.win_start = 0\n        else:\n            self.win_start -= left.start()\n\
    \        if right is None:\n            self.win_end = len(self.string)\n    \
    \    else:\n            self.win_end += right.start() + 1\n\n    def highlight_window(self):\n\
    \        '''\n        This function takes a substring of window string,\n    \
    \    which corresponds to the window size and highlights it \n        '''\n  \
    \      win_string = self.string[self.win_start:self.win_end]\n        fin = '</b>'\n\
    \        st = '<b>'\n        for position in reversed(self.positions):\n     \
    \       end = position[2] - self.win_start\n            begin = position[1] -\
    \ self.win_start\n            win_string_one = win_string[:end] + fin + win_string[end:]\n\
    \            win_string = win_string_one[:begin] + st + win_string_one[begin:]\n\
    \        return win_string\n"
  learner_created: false
- name: test/tokenizator.py
  visible: false
  text: "import unicodedata\n\"\"\"\nTokenizator\nThis module performs the morphological\
    \ analyses of a text and extracts tokens\n\"\"\"\n    \nclass Token_Type(object):\n\
    \    \"\"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def\
    \ __init__(self,s,tp,position):\n        \"\"\"\n        Consrtuctor for token.\n\
    \        @param self: self is an odject(here:token) with attributes\n        @param\
    \ s: s is a string view of a token\n        @param tp: tp is a type of a token\n\
    \        @return: token with it's type\n        \"\"\"\n        self.s=s\n   \
    \     self.tp=tp\n        self.position=position\n        \n    def __repr__(self):\n\
    \        \"\"\"\n        The way the programm returns the final result.\n    \
    \    \"\"\"\n        return self.s+ '_' + self.tp+'_'+ str(self.position)\n\n\n\
    class Tokenizator(object):\n    \"\"\"\n    Class that returns tokens\n    \"\"\
    \"\n           \n    @staticmethod\n    def tokens_type_definition(x):\n     \
    \  \"\"\"\n       This is a static method, which defines a type of a token\n \
    \      @return: type of a token\n       \"\"\"\n       tp = 'type' \n       if\
    \ x.isalpha():\n           tp = 'alpha'\n       if x.isdigit():\n           tp\
    \ = 'digit'\n       if x.isspace():\n           tp = 'space'\n       if unicodedata.category(x)[0]\
    \ == 'P':\n           tp = 'punct'\n       return tp      \n      \n    def tokens_generator_plus_type\
    \ (self, stream):\n        \"\"\"\n        This is a generator.\n        @param\
    \ stream:stream is a text given\n        @return: tokens from the given text plus\
    \ their type\n        \"\"\"\n        if  not isinstance(stream, str):\n     \
    \       raise ValueError('Input has an unappropriate type, it should be str')\n\
    \        position=0\n        for i,c in enumerate(stream): \n            if self.tokens_type_definition(c)\
    \ != self.tokens_type_definition(stream[i-1]) and i>0:\n                tp = self.tokens_type_definition(stream[i-1])\n\
    \                s = stream[position:i]\n                position = i\n      \
    \          t = Token_Type(s,tp,position)                        \n           \
    \     yield(t)     \n        if self.tokens_type_definition(c):\n            tp\
    \ = self.tokens_type_definition(c)\n            s = stream[position:i+1]\n   \
    \         t = Token_Type(s,tp,position)\n            yield(t)\n            \n\
    \    def tokens_generator_plus_type_optimized (self, stream):\n        \"\"\"\n\
    \        This is a generator.\n        @param stream:stream is a text given\n\
    \        @return: tokens from the given text plus their type\n        \"\"\"\n\
    \        if  not isinstance(stream, str):\n            raise ValueError('Input\
    \ has an unappropriate type, it should be str')\n        position=0\n        tp_of_c=self.tokens_type_definition(stream[0])\n\
    \        for i,c in enumerate(stream): \n            if i>0 and self.tokens_type_definition(c)\
    \ != tp_of_c:\n                tp = tp_of_c\n                tp_of_c = self.tokens_type_definition(c)\n\
    \                s = stream[position:i]\n                t = Token_Type(s,tp,position)\
    \  \n                position = i\n                yield(t)    \n        tp =\
    \ self.tokens_type_definition(c)\n        s = stream[position:i+1]\n        t\
    \ = Token_Type(s,tp,position)\n        yield(t)\n        \n    def token_gen(self,\
    \ stream):\n        if not stream:\n            print('None')\n        if len(stream)\
    \ == 0:\n            yield None\n        for token in self.tokens_generator_plus_type_optimized(stream):\n\
    \            if token.tp == 'alpha' or token.tp == 'digit':\n                yield(token)\n\
    \n    def token_gen_format(self, stream):\n        token_string = ''\n       \
    \ for token in self.tokens_generator_plus_type_optimized(stream):\n          \
    \  if token.tp == 'alpha' or token.tp == 'digit':\n                token_format\
    \ = token.s+'_'+token.tp+'_'+ str(token.position)\n                token_string+=\
    \ token_format+'\\n'\n        return token_string\n"
  learner_created: false
- name: tests.py
  visible: false
  text: |
    from hstest.stage_test import *
    from hstest.test_case import TestCase
    from test.windows import Context_Window
    import re

    CheckResult.correct = lambda: CheckResult(True, '')
    CheckResult.wrong = lambda feedback: CheckResult(False, feedback)

    data1 = ['I feel sure she loves lilies. And they are so appropriate to a bride.', [[0, 22, 28]], 12, 38]
    data2 = ['I feel sure she loves lilies. And they are so appropriate to a bride.', [[0, 34, 38]], 22, 45]
    data3 = ['You know you promised to obey.', [[0, 9, 12]], 4, 21]
    data4 = ['You know you promised to obey.', [[0, 13, 21]], 9, 24]
    data5 = ['Oh, by the by, dear, I shan’t be able to go with you today. I’ve rather a headache.', [[0, 53, 58]], 49, 58]
    data6 = ['Oh, by the by, dear, I shan’t be able to go with you today. I’ve rather a headache.', [[0, 74, 82]], 72, 82]
    data7 = ['Yes, she must be clever to have obtained the position that she has.', [[0, 5, 8]], 0, 13]
    data8 = ['Yes, she must be clever to have obtained the position that she has.', [[0, 45, 53]], 41, 58]

    w1 = Context_Window(*data1)
    w2 = Context_Window(*data2)
    w3 = Context_Window(*data3)
    w4 = Context_Window(*data4)
    w5 = Context_Window(*data5)
    w6 = Context_Window(*data6)
    w7 = Context_Window(*data7)
    w8 = Context_Window(*data8)

    input1 = f"{'|'.join([str(value) for value in data1])}\n" \
             f"{'|'.join([str(value) for value in data2])}"
    input2 = f"{'|'.join([str(value) for value in data3])}\n" \
             f"{'|'.join([str(value) for value in data4])}"
    input3 = f"{'|'.join([str(value) for value in data5])}\n" \
             f"{'|'.join([str(value) for value in data6])}"
    input4 = f"{'|'.join([str(value) for value in data7])}\n" \
             f"{'|'.join([str(value) for value in data8])}"


    class SearchTest(StageTest):
        def generate(self):
            return [TestCase(stdin=input1,
                             attach=w1.get_formated_window(w2)),
                    TestCase(stdin=input2,
                             attach=w3.get_formated_window(w4)),
                    TestCase(stdin=input3,
                             attach=w5.get_formated_window(w6)),
                    TestCase(stdin=input4,
                             attach=w7.get_formated_window(w8))]

        def check(self, reply: str, clue: Any) -> CheckResult:
            clue = clue.strip()
            reply = reply.strip()

            reply = reply.split('\n')
            clue = clue.split('\n')
            clue = [line.strip() for line in clue if line]
            reply = [line.strip() for line in reply if line]
            if len(reply) != len(clue):
                return CheckResult.wrong("Wrong number of windows was printed by your program.\n"
                                         f"Expected: {len(clue)} window(s)\n"
                                         f"Found: {len(reply)} window(s)")
            for i, window in enumerate(clue):
                window = window.split("|")
                if window[0] not in reply[i]:
                    return CheckResult.wrong(f"Your output should contain the line \"{window[0]}\" for window #{i + 1}.")
                if re.sub("]\[", "", window[1]) not in reply[i]:
                    return CheckResult.wrong(
                        f"Your output should contain the token position \"{window[1]}\" for window #{i + 1}")
                if window[2] not in reply[i]:
                    return CheckResult.wrong(
                        f"Your output should contain the value of the window start \"{window[2]}\" for window #{i + 1}")
                if window[3] not in reply[i]:
                    return CheckResult.wrong(
                        f"Your output should contain the value of the window end \"{window[3]}\" for window #{i + 1}")

            return CheckResult.correct()


    if __name__ == '__main__':
        SearchTest().run_tests()
  learner_created: false
- name: test/__init__.py
  visible: false
  learner_created: false
- name: db.bak
  visible: true
  learner_created: true
- name: db.dir
  visible: true
  learner_created: true
feedback_link: https://hyperskill.org/projects/168/stages/873/implement#comment
status: Solved
feedback:
  message: Congratulations!
  time: Fri, 02 Apr 2021 07:22:46 UTC
record: 5
