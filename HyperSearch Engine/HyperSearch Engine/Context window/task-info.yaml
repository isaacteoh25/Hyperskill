type: edu
files:
- name: engine.py
  visible: true
  text: |
    # write your code here
  learner_created: false
- name: test/files/ex1/testfile1.txt
  visible: false
  text: |
    Spring had come early that year.
  learner_created: false
- name: test/files/ex2/testfile2.txt
  visible: false
  text: |
    Spring had come early that year.
    He had come on tuesday night.
  learner_created: false
- name: test/files/ex3/testfile3.txt
  visible: false
  text: |
    Spring had come early that year.
  learner_created: false
- name: test/files/ex4/testfile4.txt
  visible: false
  text: |
    You can always tell weather by sunsets.
  learner_created: false
- name: test/search.py
  visible: false
  text: "\n\"\"\"\nSearchEngine\nThis module does searching for positions of tokens\
    \ in database\n\"\"\"\nfrom test.tokenizator import Tokenizator\n\n\nclass SearchEngine(object):\n\
    \    \"\"\"\n    class SearchEngine\n    \"\"\"\n    \n    def __init__(self):\n\
    \       \n        self.tokenizator = Tokenizator()\n        \n          \n   \
    \ def get_dict (self, database, tok_str):\n        \"\"\"\n        This function\
    \ performs searching for positions of a given token\n        @param tok_str: str\
    \ containing token\n        @return: dictionary, where a key is a filename\n \
    \       and a value is a list of positions\n        \"\"\"\n        \n       \
    \ if  not isinstance(tok_str,str):\n            raise TypeError('Input has an\
    \ unappropriate type!')\n        if  not isinstance(database,dict):\n        \
    \    raise TypeError('Input has an unappropriate type!')\n\n        if tok_str\
    \ in database:\n            return database[tok_str]\n        else:\n        \
    \    return {}\n\n\n    def get_dict_many_tokens(self, database, tok_str):\n \
    \       \"\"\"\n        This function performs searching for positions of given\
    \ tokens\n        @param tok_str: str containing tokens\n        @return: dictionary,\
    \ where a key is a filename\n        and a value is a list of positions of all\
    \ tokens     \n        \"\"\"\n\n        if not isinstance(tok_str, str):\n  \
    \          raise TypeError('Input has an unappropriate type!')\n        if not\
    \ tok_str:\n            return {}\n        big_dict_files = []\n        for token\
    \ in self.tokenizator.token_gen(tok_str):\n            big_dict_files.append(self.get_dict(database,token.s))\n\
    \            \n        files = set(big_dict_files[0])    \n        for file_dict\
    \ in big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \n        output_dict = {} \n        for filename in files:\n            for token\
    \ in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n\n    \n\
    \    def get_dict_many_tokens_limit_offset(self, database, tok_str, limit, offset):\n\
    \        \"\"\"\n        This function performs searching for positions of given\
    \ tokens\n        @param tok_str: str containing tokens\n        @param limit:\
    \ number of files to be returned\n        @param offset: from which file to start\n\
    \        @return: dictionary, where a key is a filename\n        @database: a\
    \ database where we search\n        and a value is a list of positions of all\
    \ tokens     \n        \"\"\"\n\n        if not isinstance(tok_str, str):\n  \
    \          raise TypeError('Input has an unappropriate type!')\n        \n   \
    \     if not isinstance(limit, int) or not isinstance (offset, int):\n       \
    \     raise TypeError('Input has an unappropriate type!')\n        \n        if\
    \ not tok_str:\n            return {}\n        \n        if offset < 0:\n    \
    \        offset = 0\n            \n        big_dict_files = []\n        for token\
    \ in self.tokenizator.token_gen(tok_str):\n            big_dict_files.append(self.get_dict(database,\
    \ token.s))  \n            \n        files = set(big_dict_files[0])    \n    \
    \    for file_dict in big_dict_files[1:]:\n            files = files.intersection(set(file_dict))\n\
    \                \n        resulted_files = sorted(files)[offset: limit+offset]\n\
    \        output_dict = {}\n        for filename in resulted_files:\n         \
    \   for token in self.tokenizator.token_gen(tok_str):\n               output_dict.setdefault(filename,[]).extend(database[token.s][filename])\n\
    \            output_dict[filename].sort()\n        return output_dict\n    \n\
    \    def get_formated_result(self, database, tok_str, limit, offset):\n      \
    \  result = self.get_dict_many_tokens_limit_offset(database, tok_str, limit, offset)\n\
    \        formated_result = '; '.join([f'{key}: {value}' for key, value in result.items()])\n\
    \        return formated_result\n     \n"
  learner_created: false
- name: test/indexer.py
  visible: false
  text: "\"\"\"\nIndexator\nThis module performs indexing of a text in a given file\n\
    \"\"\"\nfrom test.tokenizator import Tokenizator\nfrom functools import total_ordering\n\
    \nclass Position(object):\n    \"\"\"\n    Class Position\n    Cointains positions\
    \ of each token\n    \"\"\"\n\n    def __init__(self, start, end):\n        \"\
    \"\"\n        @param start: position on the 1st element of a token\n        @param\
    \ end: position on the last element of a token\n        \"\"\" \n        self.start\
    \ = start\n        self.end = end\n        \n    def __eq__(self, position):\n\
    \      \n        return self.start == position.start and self.end == position.end\n\
    \n    def __repr__(self):\n\n        return str(self.start) + ',' + str(self.end)\n\
    \n@total_ordering\nclass Position_Plus(Position):\n    \"\"\"\n    Class Position\n\
    \    Cointains positions of each token\n    \"\"\"\n\n    def __init__(self, lnumber,\
    \ start, end):\n        \"\"\"\n        @param start: position on the 1st element\
    \ of a token\n        @param end: position on the last element of a token\n  \
    \      @param lnumber: number of a line in a given text\n        \"\"\"\n    \
    \    \n        self.start = start\n        self.end = end\n        self.lnumber\
    \ = lnumber\n        \n    def __eq__(self, position):\n\n        return self.lnumber\
    \ == position.lnumber and self.start == position.start and self.end == position.end\
    \ \n\n    def __lt__(self, other_pos):\n        '''\n        This function compares\
    \ two positions, i.e. their parameters\n        and returns the result of this\
    \ comparison\n        @param other_pos: position that is to be compared with self\n\
    \        @return: comparison_result, i.e. True or False if one position less than\
    \ another\n        '''\n        return ((self.lnumber < other_pos.lnumber)\n \
    \               or ((self.lnumber == other_pos.lnumber)\n                and (self.start\
    \ < other_pos.start)))\n\n    def __repr__(self):\n\n        return str(self.lnumber)\
    \ + ', '+ str(self.start) + ', ' + str(self.end)\n          \n        \nclass\
    \ Indexer_Dict(object):\n    def __init__(self):\n        \n        self.tokenizator\
    \ = Tokenizator()\n        \n    \n    def get_index_dict(self, filename):\n \
    \       \"\"\"\n        This function performs indexing of a text in a given file\n\
    \        \"\"\"\n        if  not isinstance(filename, str):\n            raise\
    \ TypeError('Input has an unappropriate type!')\n        my_file = open(filename)\n\
    \        output_dict = {}\n        for lnumber,line in enumerate(my_file):\n \
    \           if not line:\n                lnumber+=1\n            for token in\
    \ self.tokenizator.token_gen(line):\n                start = token.position\n\
    \                end = start + len(token.s)\n                pos = Position_Plus(lnumber,\
    \ start, end)\n                output_dict.setdefault(token.s, {}).setdefault(filename,\
    \ []).append(pos)\n            lnumber+=1\n        my_file.close()\n        return\
    \ output_dict\n    \n    def get_formated_result(self, filename):\n        result\
    \ = self.get_index_dict(filename)\n        formated_result = '; '.join([f'{key}:\
    \ {value}' for key, value in result.items()])\n        return formated_result\n"
  learner_created: false
- name: test/windows.py
  visible: false
  text: "\"\"\"\nContext_Windows\nThis module returns context windows for each query\
    \ word\n\"\"\"\nfrom test.tokenizator import Tokenizator\nimport re\n\nPATTERN_RIGHT\
    \ = re.compile(r'[\\.!?] [A-ZА-Яa-zа-я]') \nPATTERN_LEFT = re.compile(r'[A-ZА\
    -Яa-zа-я] [\\.!?]')\n\nclass Context_Window(object):\n    \"\"\"\n    class Context_Window\
    \ \n    \"\"\"\n    tokenizator = Tokenizator()\n    \n    def __init__(self,\
    \ string, positions, win_start, win_end):\n        \"\"\"\n        Constructor\
    \ of a context window\n        @param positions: positions of tokens\n       \
    \ @param string: string representation of a token\n        @param win_start: position\
    \ where window starts\n        @param win_end: position where window ends\n  \
    \      \"\"\"\n\n        self.string = string\n        self.positions = positions\n\
    \        self.win_start = win_start\n        self.win_end = win_end\n\n    def\
    \ __eq__(self, window):\n\n         return self.string == window.string and self.positions\
    \ == window.positions \\\n                and self.win_start == window.win_start\
    \ and self.win_end == window.win_end\n        \n    def __repr__(self):\n    \
    \    \n        return str(self.string) + ' ' + str(self.positions) + ' ' + str(self.win_start)\
    \ + ' ' + str(self.win_end)\n        \n    @classmethod\n    def get_window(cls,\
    \ filename, position, win_size):\n        \"\"\"\n        This function returns\
    \ a context window of a given token's position\n        @param filename: a name\
    \ of a file where token is to be found\n        @param position: a position of\
    \ a token\n        @param win_size: desirable size of the context window\n   \
    \     @return: a context window\n        \"\"\"\n        if not isinstance(filename,\
    \ str) or not isinstance(win_size, int):\n            raise TypeError('Input has\
    \ an unappropriate type! %s, %s' % (filename, win_size))\n        positions =\
    \ []\n        positions.append(position)\n        win_end = 0\n        win_start\
    \ = 0\n        string = None\n        str_num = position[0]\n        my_file =\
    \ open(filename)\n        for lnumber,my_string in enumerate(my_file):\n     \
    \       if lnumber == str_num:\n                string = my_string\n         \
    \       break\n\n        if string == None:\n            my_file.close() \n  \
    \          raise TypeError('This string was not found!')\n            \n     \
    \   for tok_num,token in enumerate (cls.tokenizator.token_gen(string[position[1]:])):\n\
    \            if tok_num == 0:\n                win_end = position[2]\n       \
    \     if tok_num == win_size:\n                win_end = token.position + len(token.s)\
    \ + position[1]\n                break\n            \n        for tok_num,token\
    \ in enumerate (cls.tokenizator.token_gen(string[:position[2]][::-1])):\n    \
    \        if tok_num == win_size:\n                win_start = position[2] - token.position\
    \ - len(token.s)   \n                break\n            \n        my_file.close()\
    \    \n        return cls(string, positions, win_start, win_end)\n    \n    @classmethod\n\
    \    def get_formated_result(cls, filename, position, win_size):\n        if win_size\
    \ != 0:\n            result = cls.get_window(filename, position, win_size)\n \
    \           string = result.string\n            positions = str(result.positions)\n\
    \            start = str(result.win_start)\n            end = str(result.win_end)\n\
    \            formated_result = string.strip()+'|'+positions+'|'+start+'|'+end\n\
    \            return formated_result\n        else:\n            return ''\n  \
    \  \n    def is_crossed(self, window_B):\n        '''\n        This function checks\
    \ if windows are crossed\n        @param window_B: the second window\n       \
    \ @return: True or False\n        '''\n        if not isinstance(window_B, Context_Window):\n\
    \            raise TypeError('Input has an unappropriate type!')\n        if self.win_start\
    \ <= window_B.win_end and self.win_end >= window_B.win_start and self.positions[0].lnumber\
    \ == window_B.positions[0].lnumber:\n            return True\n        if self.win_start\
    \ == window_B.win_start and self.win_end == window_B.win_end and self.positions[0].lnumber\
    \ == window_B.positions[0].lnumber:\n            return True\n        else:\n\
    \            return False\n        \n    def get_united_window(self, window_B):\n\
    \        \n        '''\n        This function unites two windows\n        @param\
    \ window_B: the second window\n        It changes self so that is has new positions\
    \ and returns nothing!!\n        '''\n        \n        if not isinstance(window_B,\
    \ Context_Window):\n            raise TypeError('Input has an unappropriate type!')\n\
    \        \n        self.positions.extend(window_B.positions)\n        self.win_start\
    \ = min(window_B.win_start,self.win_start)\n        self.win_end = max(window_B.win_end,self.win_end)\n\
    \       \n\n    def extend_window(self):\n        '''\n        This function extends\
    \ a given window to sentence\n        @return: an extended window\n        '''\
    \ \n        to_right = self.string[self.win_end:]\n        to_left = self.string[:self.win_start+1][::-1]\n\
    \        left = PATTERN_LEFT.search(to_left)\n        right = PATTERN_RIGHT.search(to_right)\n\
    \        if left is None:\n            self.win_start = 0\n        else:\n   \
    \         self.win_start -= left.start()\n        if right is None:\n        \
    \    self.win_end = len(self.string)\n        else:\n            self.win_end\
    \ += right.start() + 1\n\n    def highlight_window(self):\n        '''\n     \
    \   This function takes a substring of window string,\n        which corresponds\
    \ to the window size and highlights it \n        '''\n        win_string = self.string[self.win_start:self.win_end]\n\
    \        fin = '</b>'\n        st = '<b>'\n        for position in reversed(self.positions):\n\
    \            end = position.end - self.win_start\n            begin = position.start\
    \ - self.win_start\n            win_string_one = win_string[:end] + fin + win_string[end:]\n\
    \            win_string = win_string_one[:begin] + st + win_string_one[begin:]\n\
    \        return win_string\n"
  learner_created: false
- name: test/tokenizator.py
  visible: false
  text: "import unicodedata\n\"\"\"\nTokenizator\nThis module performs the morphological\
    \ analyses of a text and extracts tokens\n\"\"\"\nclass Token(object):\n    \"\
    \"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def __init__(self,position,s):\n\
    \        \"\"\"\n        Consrtuctor for token.\n        @param self: self is\
    \ an odject(here:token) with attributes\n        @param position: position is\
    \ an index of the first element of token\n        @param s: s is a string view\
    \ of a token\n        @return: token\n        \"\"\"\n        self.position=position\
    \  \n        self.s=s\n        \n    def __repr__(self):\n        \"\"\"\n   \
    \     The way the programm returns the final result.\n        \"\"\"\n       \
    \ return self.s+'_'+str(self.position)\n    \n\n    \nclass Token_Type(Token):\n\
    \    \"\"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def\
    \ __init__(self,s,tp,position):\n        \"\"\"\n        Consrtuctor for token.\n\
    \        @param self: self is an odject(here:token) with attributes\n        @param\
    \ s: s is a string view of a token\n        @param tp: tp is a type of a token\n\
    \        @return: token with it's type\n        \"\"\"\n        self.s=s\n   \
    \     self.tp=tp\n        self.position=position\n        \n    def __repr__(self):\n\
    \        \"\"\"\n        The way the programm returns the final result.\n    \
    \    \"\"\"\n        return self.s+ '_' + self.tp+'_'+ str(self.position)\n  \
    \      \n    \n    \nclass Tokenizator(object):\n    \"\"\"\n    Class that returns\
    \ tokens\n    \"\"\"\n    def tokenize(self,stream):\n        \"\"\"\n       \
    \ This is a function.\n        @param stream:stream is a text given\n        @return:\
    \ a list of tokens from the given text\n        \"\"\"\n        if  not isinstance(stream,str):\n\
    \            raise ValueError('Input has an unappropriate type, it should be str')\n\
    \        tokensback = []\n        for i,c in enumerate(stream): \n           \
    \ if c.isalpha() and (not stream[i-1].isalpha() or i==0):   \n               \
    \ position=i\n            if not c.isalpha()and i>0 and stream[i-1].isalpha():\
    \   \n                s=stream[position:i]\n                t=Token(position,s)\
    \                        \n                tokensback.append(t)     \n       \
    \ if c.isalpha():                   \n            s=stream[position:i+1]\n   \
    \         t=Token(position,s)\n            tokensback.append(t)\n        return\
    \ tokensback\n    \n    def tokens_generator(self, stream):\n        \"\"\"\n\
    \        This is a generator.\n        @param stream:stream is a text given\n\
    \        @return: tokens from the given text\n        \"\"\"\n        if  not\
    \ isinstance(stream, str):\n            raise ValueError('Input has an unappropriate\
    \ type, it should be str')\n        for i,c in enumerate(stream): \n         \
    \   if c.isalpha() and (not stream[i-1].isalpha() or i == 0):   \n           \
    \     position = i\n            if not c.isalpha()and i>0 and stream[i-1].isalpha():\
    \   \n                s = stream[position:i]\n                t = Token(position,\
    \ s)                        \n                yield(t)        \n        if c.isalpha():\
    \                   \n            s = stream[position:i+1]\n            t = Token(position,\
    \ s)\n            yield(t)\n           \n    @staticmethod\n    def tokens_type_definition(x):\n\
    \       \"\"\"\n       This is a static method, which defines a type of a token\n\
    \       @return: type of a token\n       \"\"\"\n       tp = 'type' \n       if\
    \ x.isalpha():\n           tp = 'alpha'\n       if x.isdigit():\n           tp\
    \ = 'digit'\n       if x.isspace():\n           tp = 'space'\n       if unicodedata.category(x)[0]\
    \ == 'P':\n           tp = 'punct'\n       return tp      \n      \n    def tokens_generator_plus_type\
    \ (self, stream):\n        \"\"\"\n        This is a generator.\n        @param\
    \ stream:stream is a text given\n        @return: tokens from the given text plus\
    \ their type\n        \"\"\"\n        if  not isinstance(stream, str):\n     \
    \       raise ValueError('Input has an unappropriate type, it should be str')\n\
    \        position=0\n        for i,c in enumerate(stream):\n            if self.tokens_type_definition(c)\
    \ != self.tokens_type_definition(stream[i-1]) and i>0:\n                tp = self.tokens_type_definition(stream[i-1])\n\
    \                s = stream[position:i]\n                position = i\n      \
    \          t = Token_Type(s,tp,position)                        \n           \
    \     yield(t)     \n        if self.tokens_type_definition(c):\n            tp\
    \ = self.tokens_type_definition(c)\n            s = stream[position:i+1]\n   \
    \         t = Token_Type(s,tp,position)\n            yield(t)\n            \n\
    \    def tokens_generator_plus_type_optimized (self, stream):\n        \"\"\"\n\
    \        This is a generator.\n        @param stream:stream is a text given\n\
    \        @return: tokens from the given text plus their type\n        \"\"\"\n\
    \        if  not isinstance(stream, str):\n            raise ValueError('Input\
    \ has an unappropriate type, it should be str')\n        position=0\n        tp_of_c=self.tokens_type_definition(stream[0])\n\
    \        for i,c in enumerate(stream): \n            if i>0 and self.tokens_type_definition(c)\
    \ != tp_of_c:\n                tp = tp_of_c\n                tp_of_c = self.tokens_type_definition(c)\n\
    \                s = stream[position:i]\n                t = Token_Type(s,tp,position)\
    \  \n                position = i\n                yield(t)    \n        tp =\
    \ self.tokens_type_definition(c)\n        s = stream[position:i+1]\n        t\
    \ = Token_Type(s,tp,position)\n        yield(t)\n        \n    def token_gen(self,\
    \ stream):\n        if not stream:\n            print('None')\n        if len(stream)\
    \ == 0:\n            yield None\n        for token in self.tokens_generator_plus_type_optimized(stream):\n\
    \            if token.tp == 'alpha' or token.tp == 'digit':\n                yield(token)\n\
    \n    def token_gen_format(self, stream):\n        token_string = ''\n       \
    \ for token in self.tokens_generator_plus_type_optimized(stream):\n          \
    \  if token.tp == 'alpha' or token.tp == 'digit':\n                token_format\
    \ = token.s+'_'+token.tp+'_'+ str(token.position)\n                token_string+=\
    \ token_format+'\\n'\n        return token_string\n"
  learner_created: false
- name: tests.py
  visible: false
  text: |
    from hstest.stage_test import *
    from hstest.test_case import TestCase
    from test.indexer import Indexer_Dict
    from test.windows import Context_Window
    import re
    import os

    CheckResult.correct = lambda: CheckResult(True, '')
    CheckResult.wrong = lambda feedback: CheckResult(False, feedback)

    x = Context_Window('string', 'positions', 'win_start', 'win_end')
    ind = Indexer_Dict()

    dir1 = os.path.join('test', 'files', "ex1")
    dir2 = os.path.join('test', 'files', "ex2")
    dir3 = os.path.join('test', 'files', "ex3")
    dir4 = os.path.join('test', 'files', "ex4")

    file1 = 'testfile1.txt'
    file2 = 'testfile2.txt'
    file3 = 'testfile3.txt'
    file4 = 'testfile4.txt'

    pos1 = [0, 0, 6]
    pos2 = [1, 7, 11]
    pos3 = [0, 27, 31]
    pos4 = [0, 15, 19]

    window_size1 = 2
    window_size2 = 1
    window_size3 = 3
    window_size4 = 0

    input1 = f"{dir1}\n{file1};{','.join([str(value) for value in pos1])};{window_size1}"
    input2 = f"{dir2}\n{file2};{','.join([str(value) for value in pos2])};{window_size2}"
    input3 = f"{dir3}\n{file3};{','.join([str(value) for value in pos3])};{window_size3}"
    input4 = f"{dir4}\n{file4};{','.join([str(value) for value in pos4])};{window_size4}"


    class SearchTest(StageTest):
        def generate(self):
            return [TestCase(stdin=input1, attach=x.get_formated_result(os.path.join(dir1, file1),
                                                                        pos1, window_size1)),
                    TestCase(stdin=input2, attach=x.get_formated_result(os.path.join(dir2, file2),
                                                                        pos2, window_size2)),
                    TestCase(stdin=input3, attach=x.get_formated_result(os.path.join(dir3, file3),
                                                                        pos3, window_size3)),
                    TestCase(stdin=input4, attach=x.get_formated_result(os.path.join(dir4, file4),
                                                                        pos4, window_size4))]

        def check(self, reply: str, clue: Any) -> CheckResult:
            clue = clue.strip()
            reply = reply.strip()
            if clue == '':
                if reply != clue:
                    return CheckResult.wrong('The window should be an empty string, \n'
                                             'while your program printed \"%s\".)' % reply)
                else:
                    return CheckResult.correct()

            clue = clue.split('|')

            if clue[0] not in reply:
                return CheckResult.wrong(f"Your output should contain the line \"{clue[0]}\".")
            if re.sub("]\[", "", clue[1]) not in reply:
                return CheckResult.wrong(f"Your output should contain the token position \"{clue[1]}\"")
            if clue[2] not in reply:
                return CheckResult.wrong(f"Your output should contain the value of the window start \"{clue[2]}\"")
            if clue[3] not in reply:
                return CheckResult.wrong(f"Your output should contain the value of the window end \"{clue[3]}\"")

            return CheckResult.correct()


    if __name__ == '__main__':
        SearchTest().run_tests()
  learner_created: false
- name: test/__init__.py
  visible: false
  learner_created: false
- name: db.bak
  visible: true
  learner_created: true
- name: db.dir
  visible: true
  learner_created: true
feedback_link: https://hyperskill.org/projects/168/stages/872/implement#comment
status: Solved
feedback:
  message: Congratulations!
  time: Fri, 02 Apr 2021 07:21:33 UTC
record: 4
