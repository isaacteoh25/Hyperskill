type: edu
custom_name: Tokenize me!
files:
- name: test/files/testfile1.txt
  visible: false
  text: |2
     In her face were too sharply blended the delicate features of her mother.
  learner_created: false
- name: test/files/testfile2.txt
  visible: false
  text: |
    She made @ pretty picture...
  learner_created: false
- name: test/files/testfile3.txt
  visible: false
  text: |
    Her!!!eyes^were%her|own)))123
  learner_created: false
- name: test/tokenizator.py
  visible: false
  text: "import unicodedata\n\"\"\"\nTokenizator\nThis module performs the morphological\
    \ analyses of a text and extracts tokens\n\"\"\"\n\nclass Token_Type(object):\n\
    \    \"\"\"\n    Class of tokens taken from a given text\n    \"\"\"\n    def\
    \ __init__(self,s,tp,position):\n        \"\"\"\n        Consrtuctor for token.\n\
    \        @param self: self is an odject(here:token) with attributes\n        @param\
    \ s: s is a string view of a token\n        @param tp: tp is a type of a token\n\
    \        @return: token with it's type\n        \"\"\"\n        self.s=s\n   \
    \     self.tp=tp\n        self.position=position\n        \n    def __repr__(self):\n\
    \        \"\"\"\n        The way the programm returns the final result.\n    \
    \    \"\"\"\n        return self.s+ '_' + self.tp+'_'+ str(self.position)\n  \
    \  \n    \nclass Tokenizator(object):\n    \"\"\"\n    Class that returns tokens\n\
    \    \"\"\"\n           \n    @staticmethod\n    def tokens_type_definition(x):\n\
    \       \"\"\"\n       This is a static method, which defines a type of a token\n\
    \       @return: type of a token\n       \"\"\"\n       tp = 'type' \n       if\
    \ x.isalpha():\n           tp = 'alpha'\n       if x.isdigit():\n           tp\
    \ = 'digit'\n       if x.isspace():\n           tp = 'space'\n       if unicodedata.category(x)[0]\
    \ == 'P':\n           tp = 'punct'\n       return tp      \n\n    def tokens_generator(self,\
    \ stream):\n        \"\"\"\n        This is a generator.\n        @param stream:stream\
    \ is a text given\n        @return: tokens from the given text plus their type\n\
    \        \"\"\"\n        if  not isinstance(stream, str):\n            raise ValueError('Input\
    \ has an unappropriate type, it should be str')\n        position=0\n        tp_of_c=self.tokens_type_definition(stream[0])\n\
    \        for i,c in enumerate(stream):\n            if i>0 and self.tokens_type_definition(c)\
    \ != tp_of_c:\n                tp = tp_of_c\n                tp_of_c = self.tokens_type_definition(c)\n\
    \                s = stream[position:i]\n                t = Token_Type(s,tp,position)\
    \  \n                position = i\n                yield(t)    \n        tp =\
    \ self.tokens_type_definition(c)\n        s = stream[position:i+1]\n        t\
    \ = Token_Type(s,tp,position)\n        yield(t)\n\n    def token_gen_format(self,\
    \ stream):\n        token_string = ''\n        for token in self.tokens_generator(stream):\n\
    \            if token.tp == 'alpha' or token.tp == 'digit':\n                token_format\
    \ = token.s + '_' + token.tp + '_' + str(token.position)\n                token_string\
    \ += token_format+'\\n'\n        return token_string\n"
  learner_created: false
- name: tests.py
  visible: false
  text: |
    from hstest.stage_test import *
    from hstest.test_case import TestCase
    from test.tokenizator import Tokenizator
    import os

    CheckResult.correct = lambda: CheckResult(True, '')
    CheckResult.wrong = lambda feedback: CheckResult(False, feedback)
    x = Tokenizator()
    input_0 = ' In her face were too sharply blended the delicate features of her mother.'
    input_1 = 'She made @ pretty picture...'
    input_2 = 'Her!!!eyes^were%her|own)))123'
    file_1 = os.path.join('test', 'files', 'testfile1.txt')
    file_2 = os.path.join('test', 'files', 'testfile2.txt')
    file_3 = os.path.join('test', 'files', 'testfile3.txt')


    class TokenizerTest(StageTest):
        def generate(self):
            return [TestCase(stdin=file_1, attach=x.token_gen_format(input_0)),
                    TestCase(stdin=file_2, attach=x.token_gen_format(input_1)),
                    TestCase(stdin=file_3, attach=x.token_gen_format(input_2))]

        def check(self, reply: str, clue: Any) -> CheckResult:
            clue = clue.strip()
            reply = reply.strip()
            clue = clue.split('\n')
            reply = reply.split('\n')
            clue = [line.strip() for line in clue if line]
            reply = [line.strip() for line in reply if line]
            if len(clue) != len(reply):
                return CheckResult.wrong('Wrong number of tokens found.\n'
                                         f'{len(clue)} was expected, {len(reply)} was found.')

            for i, sub in enumerate(reply):
                if sub != clue[i]:
                    return CheckResult.wrong('Some tokens are not correct.\n '
                                             '{0} was found, {1} was expected.'.format(sub, clue[i]))
            return CheckResult.correct()


    if __name__ == '__main__':
        TokenizerTest().run_tests()
  learner_created: false
- name: test/__init__.py
  visible: false
  learner_created: false
- name: engine.py
  visible: true
  text: |
    # write your code here
  learner_created: false
- name: test.txt
  visible: true
  learner_created: true
feedback_link: https://hyperskill.org/projects/168/stages/869/implement#comment
status: Solved
feedback:
  message: Congratulations!
  time: Fri, 02 Apr 2021 07:14:26 UTC
record: 1
